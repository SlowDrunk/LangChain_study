/**
 * 4.2 ä½¿ç”¨æ¨¡æ¿æç¤ºè¯ç¤ºä¾‹ - FewShotChatMessagePromptTemplate
 * æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ FewShotChatMessagePromptTemplate åˆ›å»ºåŒ…å«ç¤ºä¾‹çš„èŠå¤©æ¶ˆæ¯æ¨¡æ¿
 */
import { ChatOpenAI } from "@langchain/openai";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
  MessagesPlaceholder,
  FewShotChatMessagePromptTemplate,
  PromptTemplate
} from "@langchain/core/prompts";
import { HumanMessage, SystemMessage, ToolMessage } from "@langchain/core/messages";
import dotenv from "dotenv";

dotenv.config();

async function example2() {
  console.log("=== ç¤ºä¾‹ 4.2: FewShotChatMessagePromptTemplate ===\n");
  try {
    // æ£€æŸ¥ API Key
    if (!process.env.OPENAI_API_KEY) {
      console.error("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡");
      return;
    }
    const modelConfig = {
      modelName: process.env.OPENAI_MODEL_NAME || "gpt-3.5-turbo",
      temperature: process.env.OPENAI_TEMPERATURE ? parseFloat(process.env.OPENAI_TEMPERATURE) : 0.7,
      apiKey: process.env.OPENAI_API_KEY,
    };
    // åˆ›å»º OpenAI èŠå¤©æ¨¡å‹å®ä¾‹
    const model = new ChatOpenAI(modelConfig);

    const sysMsg = new SystemMessage({
      content: 'æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘çš„åå­—å«è´¾ç»´æ–¯'
    })
    const humanMsg1 = new HumanMessage({
      content: 'è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼'
    })

    const messages = [
      sysMsg,
      humanMsg1
    ]
    console.log("ğŸ’¡å¼€å§‹æµå¼ä¼ è¾“......")
    const response = await model.invoke(messages)
    // for await (const chunk of response) {
    //   console.log(chunk.text)
    // }
    console.log("ğŸ’¡æµå¼ä¼ è¾“å®Œæˆ......")


    // console.log("\nğŸ’¬ æ¨¡å‹å›ç­”:");
    console.log(response.content);
  } catch (error) {
    console.error("âŒ å‘ç”Ÿé”™è¯¯:", error.message);
    console.error(error.stack);
  }
}

// è¿è¡Œç¤ºä¾‹
example2();
